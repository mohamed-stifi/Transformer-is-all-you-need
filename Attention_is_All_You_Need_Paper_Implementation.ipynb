{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![alt text](https://miro.medium.com/v2/resize:fit:856/1*ZCFSvkKtppgew3cc7BIaug.png)"
      ],
      "metadata": {
        "id": "ByfADv4x66fa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://miro.medium.com/v2/resize:fit:786/format:webp/1*LpDpZojgoKTPBBt8wdC4nQ.png)"
      ],
      "metadata": {
        "id": "zWht1ANA8a2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy as np\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "uWHi0i7c66Fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding and positional Encoder"
      ],
      "metadata": {
        "id": "unj4wX_VFK_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):      # embedding_dim = d_model in the orignale paper\n",
        "        '''\n",
        "        * The weights of the embedding layer are represented by a matrix that maps each word in the vocabulary to a vector in the embedding space.\n",
        "        - vocab_size: is the number of words in the vocabulary\n",
        "        - embedding_dim: is the dimension of the word embeddings\n",
        "        * The shape of the weights matrix is (vocab_size,embedding_dim).\n",
        "        '''\n",
        "        super(Embedding, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        * The input to the embedding layer is typically a batch of sequences, where each sequence is a list of token indices.\n",
        "        * For a batch of sequences (let's denote the batch size as 𝐵), the input shape would be (B,max_input_len).\n",
        "        * The output of the embedding layer is the batch of sequences with each token index replaced by its corresponding embedding vector.\n",
        "        * For a batch of sequences (with batch size B), the output shape would be (B,max_input_len,embedding_dim).\n",
        "        '''\n",
        "        return self.embedding(x)*math.sqrt(self.embedding_dim)"
      ],
      "metadata": {
        "id": "wos_tVH88-KW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embedding_dim, max_len, dropout):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.max_len = max_len\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # create matrix of shape (max_len, embedding_dim)\n",
        "        pe = torch.zeros(self.max_len, self.embedding_dim)\n",
        "\n",
        "        # create vector of shape (max_len, 1)\n",
        "        pos = torch.arange(0, max_len, dtype= torch.float).unsqueeze(1)\n",
        "\n",
        "        div_term = torch.exp(torch.arange(0, self.embedding_dim, 2).float()*(-math.log(10000.0)/self.embedding_dim))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(pos*div_term)\n",
        "        pe[:, 1::2] = torch.cos(pos*div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)   # (1, max_len, embedding_dim)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + (self.pe[:, :x.shape[1],:]).requires_grad_(False)\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "dEb0P5wh9F5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder:"
      ],
      "metadata": {
        "id": "5Y4CK6-fFChB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalisation Layer"
      ],
      "metadata": {
        "id": "rpmKkvATjaS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalisation(nn.Module):\n",
        "    def __init__(self, eps = 10**-6):\n",
        "        super(LayerNormalisation, self).__init__()\n",
        "        self.eps = eps\n",
        "        self.alpha = nn.Parameter(torch.ones(1))\n",
        "        self.bais = nn.Parameter(torch.ones(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim = True)\n",
        "        std = x.std(-1, keepdim = True)\n",
        "        return self.alpha * (x - mean) / (std + self.eps) + self.bais"
      ],
      "metadata": {
        "id": "-TnYjFnKZiIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##"
      ],
      "metadata": {
        "id": "QImIgPvslmsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, dropout):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch_s, seq_len, embedding_dim) --> (batch_s, seq_len, hidden_dim) --> (batch_s, seq_len, embedding_dim)\n",
        "        x = self.dropout(F.relu(self.linear1(x)))\n",
        "        x = self.linear2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "xXcj5mVtli8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads, dropout):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        assert embedding_dim % num_heads == 0, 'embedding_dim must be divisible by num_heads'\n",
        "        self.head_dim = embedding_dim // num_heads\n",
        "        self.W_Q = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.W_K = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.W_V = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.W_O = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(Q, K, V, mask, dropout = None):\n",
        "        \"\"\"\n",
        "            Q, K, V: (batch, num_heads, seq_len, head_dim)\n",
        "        \"\"\"\n",
        "        d_k = Q.shape[-1]\n",
        "\n",
        "        # (batch, num_heads, seq_len, head_dim) @ (batch, num_heads, head_dim, seq_len) --> (batch, num_heads, seq_len, seq_len)\n",
        "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attention_weights = scores.softmax(dim = -1)\n",
        "        if dropout is not None:\n",
        "            attention_weights = dropout(attention_weights)\n",
        "\n",
        "        # (batch, num_heads, seq_len, seq_len) @ (batch, num_heads, seq_len, head_dim) --> (batch, num_heads, seq_len, head_dim)\n",
        "        return (attention_weights @ V), attention_weights\n",
        "\n",
        "    def forward(self, Q, K, V, mask = None):\n",
        "        query = self.W_Q(Q)      # (batch, seq_len, embedding_dim) --> (batch, seq_len, embedding_dim)\n",
        "        key = self.W_K(K)      # (batch, seq_len, embedding_dim) --> (batch, seq_len, embedding_dim)\n",
        "        value = self.W_V(V)      # (batch, seq_len, embedding_dim) --> (batch, seq_len, embedding_dim)\n",
        "\n",
        "        # (batch, seq_len, embedding_dim) -->  # (batch, num_heads, seq_len, head_dim)\n",
        "        query = query.view(query.shape[0], -1, self.num_heads, self.head_dim).permute(0,2,1,3)\n",
        "\n",
        "        # (batch, seq_len, embedding_dim) -->  # (batch, num_heads, seq_len, head_dim)\n",
        "        key = key.view(key.shape[0], -1, self.num_heads, self.head_dim).permute(0,2,1,3)\n",
        "\n",
        "        # (batch, seq_len, embedding_dim) -->  # (batch, num_heads, seq_len, head_dim)\n",
        "        value = value.view(value.shape[0], -1, self.num_heads, self.head_dim).permute(0,2,1,3)\n",
        "\n",
        "        # x: (batch, num_heads, seq_len, head_dim)\n",
        "        x, self.attention_weights = MultiHeadAttention.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "        # (batch, num_heads, seq_len, head_dim) --> (batch, seq_len, num_heads, head_dim) --> (batch, seq_len, embedding_dim)\n",
        "        x = x.permute(0,2,1,3).contiguous().view(x.shape[0], -1, self.embedding_dim)\n",
        "\n",
        "        # (batch, seq_len, embedding_dim) --> (batch, seq_len, embedding_dim)\n",
        "        return self.W_O(x)"
      ],
      "metadata": {
        "id": "EqDOQ7Kcnqss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "    def __init__(self, dropout):\n",
        "        super(ResidualConnection, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = LayerNormalisation()\n",
        "\n",
        "    def forward(self, x, subLayer):\n",
        "        return x + self.dropout(self.norm(subLayer(x)))   # self.norm(subLayer(x)) or subLayer(self.norm(x))"
      ],
      "metadata": {
        "id": "KZahjT8EIYf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, self_attention_block, feed_forward_block, dropout):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "\n",
        "        self.residual_connection = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
        "\n",
        "    def forward(self, x, src_mask = None):\n",
        "        x = self.residual_connection[0](x, lambda x: self.self_attention_block(x,x,x, src_mask))\n",
        "        x = self.residual_connection[1](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "nD_g-YikJ_gA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, layers: nn.ModuleList):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalisation()\n",
        "\n",
        "    def forward(self, x, src_mask = None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, src_mask)\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "Kdc1sxTsCGIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, self_attention_block, cross_attention_block, feed_forward_block, dropout):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "\n",
        "        self.residual_connection = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        x = self.residual_connection[0](x, lambda x: self.self_attention_block(x,x,x, tgt_mask))\n",
        "        x = self.residual_connection[1](x, lambda x: self.self_attention_block(x, encoder_output, encoder_output, src_mask))\n",
        "        x = self.residual_connection[2](x, self.feed_forward_block)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "BX0k1_CPDaUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, layers: nn.ModuleList):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalisation()\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "8Kpzi8m2HmFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectionLayer(nn.Module):\n",
        "    def __init__(self, embedding_dim, vocab_size):\n",
        "        super(ProjectionLayer, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.projection = nn.Linear(embedding_dim, vocab_size)\n",
        "    def forward(self, x):\n",
        "        # (batch, seq_len, embedding_dim) --> (batch, seq_len, vocab_size)\n",
        "        return torch.log_softmax(self.projection(x), dim = -1)"
      ],
      "metadata": {
        "id": "vsVtPA6eI5zW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder, decoder, src_embedding, tgt_embedding, src_pos, tgt_pos, projection_layer):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embedding = src_embedding\n",
        "        self.tgt_embedding = tgt_embedding\n",
        "        self.src_pos = src_pos\n",
        "        self.tgt_pos = tgt_pos\n",
        "        self.projection_layer = projection_layer\n",
        "\n",
        "    def encode(self, x, src_mask):\n",
        "        x = self.src_embedding(x)\n",
        "        x = self.src_pos(x)\n",
        "        x = self.encoder(x)\n",
        "        return x\n",
        "\n",
        "    def decode(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        x = self.tgt_embedding(x)\n",
        "        x = self.tgt_pos(x)\n",
        "        x = self.decoder(x, encoder_output, src_mask, tgt_mask)\n",
        "        return x\n",
        "\n",
        "    def projection(self, x):\n",
        "        return self.projection_layer(x)"
      ],
      "metadata": {
        "id": "TyY1xwQnLEUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer(src_vocab_size, tgt_vocab_size, src_seq_len, tgt_sqe_len, embedding_dim = 512, num_heads = 8, num_hidden = 2048, num_layers = 6, dropout = 0.1):\n",
        "    # create Embedding layers\n",
        "    src_embedding = Embedding(src_vocab_size, embedding_dim)\n",
        "    tgt_embedding = Embedding(tgt_vocab_size, embedding_dim)\n",
        "\n",
        "    # create positional encoding layer\n",
        "    src_pos = PositionalEncoding(embedding_dim, src_seq_len, dropout)\n",
        "    tgt_pos = PositionalEncoding(embedding_dim, tgt_sqe_len, dropout)\n",
        "\n",
        "    # create the encoder blocks\n",
        "    encoder_blocks = []\n",
        "    for _ in range(num_layers):\n",
        "        encoder_self_attention_block = MultiHeadAttention(embedding_dim, num_heads, dropout)\n",
        "        encoder_feed_forward_block = FeedForward(embedding_dim, num_hidden, dropout)\n",
        "        encoder_blocks.append(EncoderBlock(encoder_self_attention_block, encoder_feed_forward_block, dropout))\n",
        "\n",
        "    # create the decoder blocks\n",
        "    decoder_blocks = []\n",
        "    for _ in range(num_layers):\n",
        "        decoder_self_attention_block = MultiHeadAttention(embedding_dim, num_heads, dropout)\n",
        "        decoder_cross_attention_block = MultiHeadAttention(embedding_dim, num_heads, dropout)\n",
        "        decoder_feed_forward_block = FeedForward(embedding_dim, num_hidden, dropout)\n",
        "        decoder_blocks.append(DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, decoder_feed_forward_block, dropout))\n",
        "\n",
        "    # create the encoder\n",
        "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
        "\n",
        "    # create the decoder\n",
        "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
        "\n",
        "    # create the projection layer\n",
        "    projection_layer = ProjectionLayer(embedding_dim, tgt_vocab_size)\n",
        "\n",
        "    # create the transformer\n",
        "    transformer = Transformer(encoder, decoder, src_embedding, tgt_embedding, src_pos, tgt_pos, projection_layer)\n",
        "\n",
        "    # initialize the parameters of the transformer\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p.data)\n",
        "\n",
        "    return transformer\n"
      ],
      "metadata": {
        "id": "dJGKRN9EV0gJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# training\n"
      ],
      "metadata": {
        "id": "5TBQEl0_pw66"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tokenizer"
      ],
      "metadata": {
        "id": "Fp6-g1JR1hol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Niwr0shcqvkL",
        "outputId": "8c9aadb5-a0a8-42bc-a2ad-7ae7e884d1e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/547.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/547.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests>=2.32.2 (from datasets)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, requests, pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-16.1.0 requests-2.32.3 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from pathlib import Path\n",
        "from typing import Any\n",
        "from torch.utils.data import Dataset, random_split, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "5uMMbY5hcnfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_sentences(ds, lang):\n",
        "    for item in ds:\n",
        "        yield item['translation'][lang]"
      ],
      "metadata": {
        "id": "Rb2OVG67t4f-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_or_build_tokenizer(config, ds, lang):\n",
        "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
        "    if not Path.exists(tokenizer_path):\n",
        "        tokenizer = Tokenizer(WordLevel(unk_token = \"[UNK]\"))\n",
        "        tokenizer.pre_tokenizer = Whitespace()\n",
        "        trainer = WordLevelTrainer(special_tokens = [\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency = 2)\n",
        "\n",
        "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer)\n",
        "        tokenizer.save(str(tokenizer_path))\n",
        "    else:\n",
        "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "X14vQSUZq44G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dataset"
      ],
      "metadata": {
        "id": "DikYsKqn1l92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_mask(size):\n",
        "    mask = torch.triu(torch.ones(1, size, size), diagonal=1).type(torch.int64)\n",
        "    return mask == 0"
      ],
      "metadata": {
        "id": "MBjTWUa4652n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BilingualDataset(Dataset):\n",
        "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.ds = ds\n",
        "        self.tokenizer_src = tokenizer_src\n",
        "        self.tokenizer_tgt = tokenizer_tgt\n",
        "        self.src_lang = src_lang\n",
        "        self.tgt_lang = tgt_lang\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        self.sos_token = torch.tensor([tokenizer_src.token_to_id('[SOS]')], dtype=torch.int64)\n",
        "        self.eos_token = torch.tensor([tokenizer_src.token_to_id('[EOS]')], dtype=torch.int64)\n",
        "        self.pad_token = torch.tensor([tokenizer_src.token_to_id('[PAD]')], dtype=torch.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, index: Any) -> Any:\n",
        "        src_target_pair = self.ds[index]\n",
        "        src_text = src_target_pair['translation'][self.src_lang]\n",
        "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
        "\n",
        "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
        "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
        "\n",
        "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2\n",
        "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
        "\n",
        "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
        "            raise ValueError(\"Sentence is too long\")\n",
        "\n",
        "        # Add SOS and EOS to the source text\n",
        "        encoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Add SOS to the decoder input\n",
        "        decoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Add EOS to the label (what we expect as output from the decoder)\n",
        "        label = torch.cat(\n",
        "            [\n",
        "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        assert encoder_input.size(0) == self.seq_len\n",
        "        assert decoder_input.size(0) == self.seq_len\n",
        "        assert label.size(0) == self.seq_len\n",
        "\n",
        "        return {\n",
        "            \"encoder_input\": encoder_input,   # ( Seq_Len)\n",
        "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),  # (1, 1, Seq_Len)\n",
        "            \"decoder_input\": decoder_input,   # ( Seq_Len)\n",
        "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),  # (1, 1, Seq_Len)\n",
        "            \"label\": label,   # ( Seq_Len)\n",
        "            \"src_text\": src_text,\n",
        "            \"tgt_text\": tgt_text,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "3H6oteZ51nkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_ds(config):\n",
        "    ds_raw = load_dataset('opus_books', f'{config[\"lang_src\"]}-{config[\"lang_tgt\"]}', split='train')\n",
        "\n",
        "    # Build tokenizers\n",
        "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config[\"lang_src\"])\n",
        "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config[\"lang_tgt\"])\n",
        "\n",
        "    # Keep 90% for training and 10% for validation\n",
        "    train_ds_size = int(0.9 * len(ds_raw))\n",
        "    val_ds_size = len(ds_raw) - train_ds_size\n",
        "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
        "\n",
        "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config[\"lang_src\"], config[\"lang_tgt\"], config['seq_len'])\n",
        "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config[\"lang_src\"], config[\"lang_tgt\"], config['seq_len'])\n",
        "\n",
        "    max_src_len = 0\n",
        "    max_tgt_len = 0\n",
        "\n",
        "    for item in ds_raw:\n",
        "        src_ids = tokenizer_src.encode(item['translation'][config[\"lang_src\"]]).ids\n",
        "        tgt_ids = tokenizer_src.encode(item['translation'][config[\"lang_tgt\"]]).ids\n",
        "\n",
        "        max_src_len = max(max_src_len, len(src_ids))\n",
        "        max_tgt_len = max(max_tgt_len, len(src_ids))\n",
        "\n",
        "    print(f\"max src len: {max_src_len},\\nmax tgt len: {max_tgt_len}\")\n",
        "\n",
        "    train_dataloader = DataLoader(train_ds, batch_size = config['batch_size'], shuffle = True)\n",
        "    val_dataloader = DataLoader(val_ds, batch_size = 1, shuffle = True)\n",
        "\n",
        "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt"
      ],
      "metadata": {
        "id": "rrjVSSL3z6AI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(config, vocsb_src_size, vocab_tgt_src):\n",
        "    model = transformer(src_vocab_size = vocsb_src_size,\n",
        "                        tgt_vocab_size = vocab_tgt_src,\n",
        "                        src_seq_len = config['seq_len'],\n",
        "                        tgt_sqe_len = config['seq_len'],\n",
        "                        embedding_dim = config['embedding_dim'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "7m5GRHYn7ivq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_config():\n",
        "    return {\n",
        "        'batch_size': 4,\n",
        "        'num_epochs': 4,\n",
        "        'lr': 0.001,\n",
        "        'seq_len':  400,\n",
        "        'embedding_dim': 512,\n",
        "        'lang_src': 'en',\n",
        "        'lang_tgt': 'it',\n",
        "        'tokenizer_file': 'tokenizer_{0}.json',\n",
        "        'model_folder': 'weights',\n",
        "        'model_filename': 'tmodel_',\n",
        "        'preload':' 1',\n",
        "        'experiment_name': 'runs/tmodel',\n",
        "    }"
      ],
      "metadata": {
        "id": "WzURSm1WMi3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weights_file_name(config, _epoch):\n",
        "    return f\"./{config['model_folder']}/{config['model_filename']}{_epoch}.pt\""
      ],
      "metadata": {
        "id": "KBQmIdU2Ox6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
        "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
        "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
        "\n",
        "    # Precompute the encoder output and reuse it for every step\n",
        "    encoder_output = model.encode(source, source_mask)\n",
        "    # Initialize the decoder input with the sos token\n",
        "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
        "    while True:\n",
        "        if decoder_input.size(1) == max_len:\n",
        "            break\n",
        "\n",
        "        # build mask for target\n",
        "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
        "\n",
        "        # calculate output\n",
        "        out = model.decode(decoder_input, encoder_output, source_mask, decoder_mask)\n",
        "\n",
        "        # get next token\n",
        "        prob = model.projection(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        decoder_input = torch.cat(\n",
        "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
        "        )\n",
        "\n",
        "        if next_word == eos_idx:\n",
        "            break\n",
        "\n",
        "    return decoder_input.squeeze(0)"
      ],
      "metadata": {
        "id": "F3BbTyKE0B3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, writer, num_examples=2):\n",
        "    model.eval()\n",
        "    count = 0\n",
        "\n",
        "\n",
        "    # If we can't get the console width, use 80 as default\n",
        "    console_width = 80\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in validation_ds:\n",
        "            count += 1\n",
        "            encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n",
        "            encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
        "\n",
        "            # check that the batch size is 1\n",
        "            assert encoder_input.size(\n",
        "                0) == 1, \"Batch size must be 1 for validation\"\n",
        "\n",
        "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
        "\n",
        "            source_text = batch[\"src_text\"][0]\n",
        "            target_text = batch[\"tgt_text\"][0]\n",
        "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
        "\n",
        "            # Print the source, target and model output\n",
        "            print_msg('-'*console_width)\n",
        "            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
        "            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
        "            print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n",
        "\n",
        "            if count == num_examples:\n",
        "                print_msg('-'*console_width)\n",
        "                break"
      ],
      "metadata": {
        "id": "nsoTfDn1lt8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(config):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    Path(config['model_folder']).mkdir(parents= True, exist_ok= True)\n",
        "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
        "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
        "\n",
        "    # tensorBord\n",
        "    writer = SummaryWriter(config['experiment_name'])\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = config['lr'], eps= 1e-9)\n",
        "\n",
        "    initial_epoch = 0\n",
        "    global_step = 0\n",
        "    if config['preload'] is not None:\n",
        "        model_filename = get_weights_file_name(config, config['preload'])\n",
        "\n",
        "        state = torch.load(model_filename)\n",
        "        initial_epoch = state['epoch']\n",
        "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
        "        global_step = state['global_step']\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index= tokenizer_src.token_to_id('[PAD]'), label_smoothing= 0.1).to(device)\n",
        "\n",
        "    for epoch in range(initial_epoch, config['num_epochs']):\n",
        "        model.train()\n",
        "        batch_iterator = tqdm(train_dataloader, desc = f'processing epoch {epoch: 02d}')\n",
        "        for batch in batch_iterator:\n",
        "            encoder_input = batch['encoder_input'].to(device)      # (B, seq_len)\n",
        "            decoder_input = batch['decoder_input'].to(device)      # (B, seq_len)\n",
        "            encoder_mask = batch['encoder_mask'].to(device)      # (B, 1, 1, seq_len)\n",
        "            decoder_mask = batch['decoder_mask'].to(device)      # (B, 1, seq_len, seq_len)\n",
        "\n",
        "            # run transformer\n",
        "            encoder_output = model.encode(encoder_input, encoder_mask)   # (B, seq_len, embedding_dim)\n",
        "            dencoder_output = model.decode(decoder_input, encoder_output, encoder_mask, decoder_mask)   # (B, seq_len, embedding_dim)\n",
        "            proj_output = model.projection(dencoder_output)   # (B, seq_len, tgt_vocab_size)\n",
        "\n",
        "            # calculat loss\n",
        "            label = batch['label'].to(device)   # (B, seq_len)\n",
        "\n",
        "            # proj_output: (B, seq_len, tgt_vocab_size) --> (B * seq_len, tgt_vocab_size)\n",
        "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
        "\n",
        "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
        "\n",
        "\n",
        "            # log the loss\n",
        "            writer.add_scalar('train loss', loss.item(), global_step)\n",
        "            writer.flush()\n",
        "\n",
        "            # backpropaget loss\n",
        "            loss.backward()\n",
        "\n",
        "            # update the weights\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            global_step += 1\n",
        "        # save the model at the end of each epoch\n",
        "        model_filename = get_weights_file_name(config, f\"{epoch: 02d}\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'global_step': global_step\n",
        "        }, model_filename)"
      ],
      "metadata": {
        "id": "5xmcCdDoP-Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = get_config()\n",
        "train_model(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "vy1vsDH3y0M4",
        "outputId": "2711c01f-c215-450f-e485-866399754457"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max src len: 309,\n",
            "max tgt len: 309\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "processing epoch  1:   0%|          | 33/7275 [00:08<32:38,  3.70it/s, loss=7.837]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-709e4a70de3b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-48-9317bb174416>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproj_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_tgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocab_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mbatch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"{loss.item():6.3f}\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mset_postfix\u001b[0;34m(self, ordered_dict, refresh, **kwargs)\u001b[0m\n\u001b[1;32m   1429\u001b[0m                                  for key in postfix.keys())\n\u001b[1;32m   1430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1431\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_postfix_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mrefresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1347\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1348\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnolock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   1493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1495\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_meter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mformat_meter\u001b[0;34m(n, total, elapsed, ncols, prefix, ascii, unit, unit_scale, rate, bar_format, postfix, unit_divisor, initial, colour, **extra_kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m             \u001b[0ml_bar\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf'{percentage:3.0f}%|'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mncols\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0ml_bar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mr_bar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the state_dict into the model and optimizer\n",
        "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
        "model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
        "\n",
        "model_filename = get_weights_file_name(config, config['preload'])\n",
        "state = torch.load(model_filename)\n",
        "\n",
        "model.load_state_dict(state['model_state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVItDqfRoOyc",
        "outputId": "ba966233-7d68-4594-c9aa-d36e70718f04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max src len: 309,\n",
            "max tgt len: 309\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_iterator = tqdm(train_dataloader, desc = f'processing epoch')\n",
        "run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: print(msg) , None, None)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWIZjhlYqoeS",
        "outputId": "d56884ae-5bdf-4953-c8d1-94edbe48743a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: But a week passed, and another, and a third, and no impression was noticeable in Society. His friends, the specialists and the scholars, sometimes – from politeness – mentioned it; his other acquaintances, not interested in learned works, did not mention it to him at all.\n",
            "    TARGET: Ma passò una settimana, ne passarono due, tre e nella società non si notava alcuna impressione; gli amici specialisti e studiosi, a volte, evidentemente per cortesia, ne cominciavano a parlare. Ma gli altri suoi conoscenti, non interessati a un libro di contenuto scientifico, non ne parlavano affatto.\n",
            " PREDICTED: — No , non vi , — disse , — — e , — e , e , con un sorriso , e , con un sorriso , e la signora Reed , e , e , con un sorriso , e con un sorriso , e con un sorriso , e la sua voce , e con un sorriso , e la sua voce , e con un sorriso di una voce , e con un sorriso di una voce di cui disse , e con la sua voce , con la sua voce , e con un sorriso , e con un sorriso , con un sorriso , e la sua voce , con un sorriso , e con la sua voce , con la sua voce , e la sua voce , con un sorriso , e la sua voce , con un sorriso , e la sua voce , e la sua voce , con un sorriso , con un sorriso , con un sorriso , con un sorriso , e la sua espressione di cui disse , e la sua voce , e con un sorriso , e la sua voce , con un sorriso , e la sua voce , e la sua voce , con un sorriso voce voce voce , con un sorriso , con un sorriso , con un sorriso , con un sorriso , e la sua voce , con un sorriso , con un sorriso , con un sorriso , con un sorriso , con un sorriso , con un sorriso , con un sorriso , e la sua voce , e la sua espressione di cui disse la sua voce voce voce voce voce voce voce di una poltrona , e la sua voce voce voce , con un sorriso , con un sorriso , con un sorriso , con un sorriso , con un sorriso , e la sua voce , con un sorriso , con un sorriso , con un sorriso , con un sorriso , con un sorriso voce voce voce voce voce voce , con un sorriso , con un sorriso , con un sorriso , con un sorriso , con un sorriso , con un sorriso , con un sorriso , con un sorriso con un sorriso , con un sorriso , con un sorriso , con un sorriso di un\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: 'She understands,' thought he, 'she knows what I am thinking about.\n",
            "    TARGET: “Lei capisce — egli pensava — sa a che cosa penso.\n",
            " PREDICTED: — No , non è nulla — disse il signor Rochester .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "twmfyRx3s_XN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}